

参考：《智能控制理论与方法——李人厚》、ppt课件——胡怀中

------



- 模糊控制

  - 模糊集合
    - 定义
      - 论域，隶属函数
      - 表示：离散空间；连续空间 $A=\begin{cases}\sum_{x_i\in X}\mu_A(x_i)/x_i\quad X\mbox{为离散对象集合}\\\int_X\mu_A(x)/x\quad X\mbox{为连续空间}\end{cases}$
      - 支集，核，交叉点，模糊单点，凸性，语言变量，正态性、模糊数
      - MF
        - 常见的一维MF：三角形，梯形，高斯型，钟形$bell(x;a,b,c)$
        - 二维MF：复合式和非复合式
        - 一维的圆柱扩展，二维的投影
    - 运算
      - 集合运算：所有的集合运算都是对MF的运算
        - 包含（子集）
        - 模糊交：$\mu_{A\cap B}=T(\mu_A(x),\mu_B(x))=\mu_A(x)\widetilde *\mu_B(x)$——T-范式（三角范式）
          - 极小($T_{min}$)，代数积($T_{ap}$)，有界积($T_{bp}$)，强积($T_{dp}$)；且有$T_{dp}\leq T_{bp}\leq T_{ap}\leq T_{min}$
        - 模糊并：$\mu_{A\cup B}=S(\mu_A(x),\mu_B(x))=\mu_A(x)\widetilde +\mu_B(x)$——T-协范式（协三角范式）/S-范式
          - 极大($S_{max}$)，代数和($S_{as}$)，有界和($S_{bs}$)，强和($S_{ds}$)；且有$S_{ds}\ge S_{bs}\ge S_{as}\ge S_{min}$
        - 模糊补：$N_s(a)=(1-a)/(1+sa),s>-1；N_w(a)=(1-a^w)^{1/w},w>0$
        - 修正运算：$A^k=\int_X[\mu_a(x)]^k/x$
          - 压缩($k=2$)，扩张($k=0.5$)，人为修正($k为任意值$)
      - 复合运算：$P(X,Z)=R(X,Y)\circ S(Y,Z);\mu_{R\circ S}=sup({\mu_R(x,y)\star\mu_S(y,z)})$
        - 当X,Y,Z为离散空间时，采用max-min，max-乘积
  - 模糊推理：离散论域可以使用矩阵来推理，也可以通过画图来推理；连续论域只能通过画图来推理
    - 由规则得到模糊关系：Mamdani-min，Larsen-乘积
    - 合成运算方法：Zadeh-最大最小，Kaufmann-最大-乘积下面的复合运算为 max-min，$A\rightarrow B解释为A与B相关​$
    - 单个前提单个规则：$\mu_{B'}(y)=\bigvee\limits_x[\mu_{A'}(x)\wedge\mu_A(x)\wedge\mu_B(y)]=w\wedge\mu_B(y)​$（注意：是在$X​$上进行max运算，以下同）
    - 多个前提单个规则：$\mu_{C'}(z)=(w_1\wedge w_2)\wedge\mu_c(z)​$
    - 多个前提多个规则：先计算每一个规则得到的MF，然后把所有MF集结(max)，得到最终结果
    - 步骤：计算兼容度；求激励强度（规则前提部分满足的程度）；求定性结果MF；求总输出MF
  - 模糊推理系统
    - 模糊化和模糊器——预滤波
      - 单点模糊器（计算简单），非单点模糊器（数据被测量噪声污染）
    - 规则库：规则表
    - 推理机：$B=A_x\circ[R^1,R^2...,R^M]=\bigcup\limits_{i=1}^{M}A_x\circ R^i$
      - 相加的规则合成法则——规则相加合成器=模糊预滤波+自适应滤波
    - 去模糊化：“艺术而非科学”
      - 极大去模糊化；最大平均去模糊化；重心或面积中心去模糊化；面积均分去模糊化高度去模糊化；修正的高度去模糊化
  - T-S推理：if $x_1$ 是 $A_1$ 和 $x_2$ 是 $A_2$ 和 ……和 $x_k$ 是 $A_k$ ，then $y=f(x)$
    - $f(x)$为常数时，零阶T-S模型
    - $f(x)$为$x_i$的线性多项式时，一阶T-S模型

- 神经网络：一种旨在模仿人脑结构及其功能的信息处理系统

  - 基础
    - 特征：并行式处理，分布式存储，容错性；自学习，自组织，自适应性
    - 功能：联想记忆，非线性映射，分类与识别，优化计算，知识处理
    - 要素
      - 节点本身的信息处理能力（数学模型-转移函数）
        - 阈值型，sigmoid（单极性，双极性-$f(x)=\frac{1-e^{-x}}{1+e^{-x}}$），分段线性，概率型($P(1)=\frac{1}{1+e^{-x/T}}$)
      - 节点与节点之间的连接（拓扑结构）
        - 层次型，互连型
        - 前馈型，反馈型
      - 相互连接的强度（通过学习来调整）
        - 有导师学习，无导师学习，死记式学习
  - 前馈神经网络
    - 感知器
      - 单层感知器训练：权值初始化；输入样本对；计算输出；根据感知器学习规则（$w=w+\eta(t-o)x^T$）调整权值；返回步骤二循环直到感知器的实际输出与期望输出相等
      - 多层感知器：隐层的引入；转移函数
    - BP算法
      - 推导
      - 实现步骤：初始化；输入训练样本计算输出；计算误差；计算各层误差信号；调整各层权值；检查是否对所有样本完成一次轮训；检查网络的总误差是否达到精度要求
      - 多层前馈网：非线性映射能力；泛化能力；容错能力
      - 局限性：平坦区域-饱和区域-节点净输入过大（sigmoid）；局部极小点
      - 改进
        - 动量项 
          - 训练过程发生振荡，收敛缓慢
          - $\Delta w(t)=\eta\delta x+\alpha\Delta w(t-1)$
        - 自适应调节学习率
          - 平坦区域学习率太小会造成收敛缓慢；误差变化剧烈的区域，学习率太大会造成训练发生振荡
          -  $\eta=\beta\eta$
        - 陡度因子
          - 平坦区域的存在是由于神经元输出进入了激活函数的饱和区
          -  $o=\frac{1}{1+e^{-net/\lambda}}$
      - 设计
        - 输入量
          - 两个基本原则：选择对输出影响大且能够检测或提取的变量；各输入变量之间互不相关或者相关性很小
          - 分类：数值变量，语言变量
        - 输出量
          - 表示：n中取1；n-1表示法；数值表示法
        - 归一化
          - 原因：各输入数据常常具有不同的物理意义和量纲；防止净输入绝对值过大进入饱和区；sigmoid函数的输出在[0,1]或[-1,1]里，教师信号必须归一化
          - [0,1]或[-1,1]
  - 自组织神经网络
    - 自组织学习：通过自动寻找样本中的内在规律和本质属性，自组织、自适应地改变网络参数与结构（竞争学习）
    - 相似性测量：欧式距离；余弦法
    - 竞争学习规则：winner take all
    - 步骤：向量归一化；寻找获胜神经元（欧式距离最小，点积最大）；网络输出与权值调整（只有获胜神经元可以调整，公式）；循环直到学习率衰减到0
    - SOM网
      - 拓扑结构：两层
      - 获胜神经元---优胜邻域：开始很大，不断收缩，最后为0
      - Kohonen学习算法
        - 初始化；接受输入；寻找获胜节点；定义优胜邻域；调整权值（公式）；结束检查（$\eta$）
  - 反馈神经网络：DHNN(Discrete Hopfield Neural Network);CHNN(Continues Hopfield Network)
    - 拓扑结构
    - 状态 $X=[x_1,x_2,...,x_n]^T$   $net_j=\sum\limits_{i=1}^n(w_{ij}x_i-T_j)\quad w_{ii}=0;w_{ij}=w_{ji}\quad x_j=sgn(net_j)$ 
    - 网络的输出 $\lim\limits_{t\rightarrow \infty}X(t)$
    - 工作方式：异步、同步
    - 稳定性
      - 网络从初态开始，若能经过有限次递归之后，其状态不再发生变化，则称该网络是稳定的
      - 若不稳定，由于每个节点的状态只有1和-1两种情况，所以不会无限发散，只会出现限幅的自持振荡。称为有限环网络
      - 如果网络状态的轨迹在某个确定的范围内变迁，但既不重复也不停止，状态变化为无穷多个，轨迹不发散到无穷远，称为浑沌现象
    - 吸引子及能量函数：网络达到稳定时的状态 $X=f(WX-T)$
      - DHNN，异步工作，$W$为对称阵，那么对任意初态，网络都最终收敛到一个吸引子（证明,$w_{ii}=0$）
      - DHNN，同步工作，$W$为非负定对称阵，对任意初态，网络都最终收敛到一个吸引子（证明）
      - 网络能量的极小状态——网络的能量井——网络的吸引子
      - 性质（证明，吸引子的定义）
        - $X$为一个吸引子，且$T=0$，任意一个节点的净输入不为0，那么$-X$也是一个吸引子
        - $X^a​$为一个吸引子，海明距离$dH(X^a,X^b)=1​$，$X^b​$一定不是吸引子
      - 吸引域：能使网络稳定在同一吸引子的所有初态的集合
        - 弱吸引（域）与强吸引（域）——异步方式
    - 权值设计——外积和法
      - 给定$P$个模式样本$X^P,x\in \left\{-1,1\right\}^n$，样本两两正交，$n>P$，那么取$W=\sum\limits_{p=1}^PX^P(X^P)^T$或者$W=\sum\limits_{p=1}^P[X^P(X^P)^T-I]$ （证明）

- 遗传算法

  - 算法
    - 编码：二进制串
    - 适应度函数
    - 遗传算子：轮盘赌，单点交叉，基本位变异
    - 运行参数：种群规模，终止进化代数，交叉概率，变异概率
  - 数学基础
    - 模式定理
      - 模式：种群个体基因串中的相似样板；模式的阶 $O(H)​$；模式的定义距 $\delta(H)​$
      - 定理：具有低阶、短定义距以及平均适应度高于种群平均适应度的模式将在子代中呈指数增长
    - 积木块假设：遗传算法通过短定义距、低阶以及高平均适应度的模式（积木块），在遗传操作下相互结合，最终接近全局最优解
  - 改进
    - 遗传欺骗问题
    - 编码方式：格雷码（解决不连续问题），浮点数编码（简单计算复杂度）
    - 排序选择（固定的选择概率），均匀变异，逆序变异
    - 控制参数：M=20-100,T=100-500,$P_c=0.4-0.9​$，$P_m=0.001-0.01​$ ——自适应遗传算法
    - 混合遗传，免疫遗传，小生境遗传，单亲遗传，并行遗传

  ------

  

- 填空题（15*1）

  - 模糊集合的表示和各种定义
  - 常见MF，圆柱扩展和投影
  - 模糊集合的运算
  - T-S推理
  - 神经网络的转移函数，分类
  - 单层感知器的学习规则
  - BP算法的可调整参数个数；误差的权空间
  - 动量项，自适应调节学习率，陡度因子
  - 输入量的表示；输出量的表示
  - 归一化
  - 相似性测量
  - 反馈网络的工作方式
  - 稳定性；有限环网络；浑沌
  - 吸引子，能量函数定义
  - 弱吸引，强吸引
  - 最优保存策略——以概率1收敛于全局最优解
  - 改进策略

- 名词解释、问答（5*5）

  - 模糊推理的步骤：四步
  - 模糊推理系统的组成：四个
  - 人工神经网络的定义，特征（3+3），功能（5个），三大要素
  - 单层感知器的训练步骤
  - 单层感知器的功能、局限性；多层感知器的功能、优点
  - BP的实现步骤
  - 标准BP算法的内在缺陷：4点
  - 动量项的提出原因，基本思想，作用
  - 自适应调节学习率的提出原因，基本思想，基本方法
  - 陡度因子的提出原因，基本思想，基本方法
  - 输入量的两个基本原则
  - 输出量的表示方法
  - 进行归一化的主要原因
  - 自组织学习定义
  - 竞争学习步骤
  - SOM网学习流程
  - 浑沌现象
  - 轮盘赌的步骤
  - 遗传算法的特点
  - 遗传算法步骤
  - 模式定理与积木块假设
  - 改进策略

- 计算题（60）

  - 模糊推理：模糊化，推理机，去模糊化
  - 感知器学习的过程
  - BP算法的推导
  - 竞争学习的过程
  - DHNN的网络演变过程（状态变化，状态转移概率）
  - 吸引子两个定理的证明
  - 吸引子两个性质的证明
  - 权值设计——外积和法——证明与计算